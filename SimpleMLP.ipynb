{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "# Licensed under the MIT license. See LICENSE.md file in the project root\n",
    "# for full license information.\n",
    "# ==============================================================================\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From C:\\Users\\Lemming\\AppData\\Local\\Temp\\ipykernel_5932\\1148016718.py:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_built_with_cuda())\n",
    "print (tf.config.list_physical_devices('GPU') )\n",
    "\n",
    "print (tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OS Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #abs_path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# def check_path(path):\n",
    "#     if not os.path.exists(path):\n",
    "#         readme_file = os.path.normpath(os.path.join(\n",
    "#             os.path.dirname(path), \"..\", \"README.md\"))\n",
    "#         raise RuntimeError(\n",
    "#             \"File '%s' does not exist. Please follow the instructions at %s to download and prepare it.\" % (path, readme_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Input Data\n",
    "\n",
    "Deserializer expects the format `sequenceId <tab> path <tab> label`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataset_label(index, path):\n",
    "# \tfoldername = os.path.basename(os.path.dirname(path))\n",
    "# \treturn str(index).zfill(5) + \"\\t\" + os.path.normpath(path) + \"\\t\" + os.path.normpath(foldername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to parse through the folders and build our labels.\n",
    "It's easy because we can just base it on directory name\n",
    "\n",
    "For each folder in the data directory, find all files with the type `.png`, `jpg`, `jpeg`, `.gif`\n",
    "\n",
    "concat the output of dataset_label into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.normpath(\"\\\\\\\\NAS12139F\\\\Business\\\\Collections\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # For each folder in the data directory, find all files with the type `.png`, `jpg`, `jpeg`, `.gif`\n",
    "# # concat the output of dataset_label into an array, then save to file\n",
    "\n",
    "# def label_builder():\n",
    "\n",
    "# \tlabel_filename = os.path.dirname(DATA_DIR) + \".txt\"\n",
    "# \tprint (\"Looking for label file: \" + label_filename)\n",
    "# \t# if file exists, return it's path\n",
    "# \tif os.path.isfile(label_filename):\n",
    "# \t\tprint (\"Found label file: \" + label_filename)\n",
    "# \t\treturn os.path.join(label_filename)\n",
    "\n",
    "# \tprint (\"Creating label file\")\n",
    "# \tlabel_array = []\n",
    "# \tclass_track = []\n",
    "# \tindex = 0\n",
    "# \t# directory tree traversal\n",
    "# \tfor root, dirs, files in os.walk(DATA_DIR):\n",
    "# \t\tfor file in files:\n",
    "# \t\t\tif file.endswith(\".png\") or file.endswith(\".jpg\") or file.endswith(\".jpeg\") or file.endswith(\".gif\"):\n",
    "# \t\t\t\tlabel_array.append(dataset_label(index, os.path.join(root, file)))\n",
    "# \t\t\t\tclass_track.append(os.path.normpath(os.path.basename(os.path.dirname(os.path.join(root, file)))))\n",
    "# \t\t\t\tindex += 1\n",
    "# \t# save labels to file\n",
    "# \twith open(label_filename, \"w\") as f:\n",
    "# \t\tfor label in label_array:\n",
    "# \t\t\tif len(label) != len(label.encode()):\n",
    "# \t\t\t\tcontinue\n",
    "# \t\t\tf.write(label)\n",
    "# \t\t\tf.write(\"\\n\")\n",
    "# \tprint (\"Label file saved to: \" + label_filename)\n",
    "# \tNUM_CLASSES = len(np.unique(class_track))\n",
    "# \tprint(\"Number of classes: \" + str(NUM_CLASSES))\n",
    "\n",
    "# \t#return path to file\n",
    "# \treturn label_filename\n",
    "\n",
    "# Generate Labels\n",
    "\n",
    "# label_builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dataset\n",
    "\n",
    "(Using Keras now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  \tlayers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  \tlayers.RandomRotation(0.2),\n",
    "  \tlayers.RandomZoom(0.1),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1338 files belonging to 10 classes.\n",
      "Using 1071 files for training.\n",
      "Found 1338 files belonging to 10 classes.\n",
      "Using 267 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<CacheDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_in = 5421\n",
    "image_size = (256, 256)\n",
    "\n",
    "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "\tDATA_DIR,\n",
    "\tvalidation_split=0.2,\n",
    "\tsubset=\"training\",\n",
    "\tseed=seed_in,\n",
    "\timage_size=image_size,\n",
    "\tbatch_size=BATCH_SIZE,\n",
    ")\n",
    "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "\tDATA_DIR,\n",
    "\tvalidation_split=0.2,\n",
    "\tsubset=\"validation\",\n",
    "\tseed=seed_in,\n",
    "\timage_size=image_size,\n",
    "\tbatch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "class_track = train_ds.class_names\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=32)\n",
    "val_ds = val_ds.prefetch(buffer_size=32)\n",
    "\n",
    "train_ds.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of classes: \" + str(len(class_track)))\n",
    "\n",
    "print(\"Classes: \" + str(class_track))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a model\n",
    "\n",
    "We'll build a small version of the Xception network. We haven't particularly tried to optimize the architecture; if you want to do a systematic search for the best model configuration, consider using KerasTuner.\n",
    "\n",
    "Note that:\n",
    "\n",
    "    We start the model with the data_augmentation preprocessor, followed by a Rescaling layer.\n",
    "    We include a Dropout layer before the final classification layer.\n",
    "\n",
    "https://keras.io/examples/vision/image_classification_from_scratch/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, name, epochs=20):\n",
    "\n",
    "   callbacks = [\n",
    "      #keras.callbacks.ModelCheckpoint(\"models/augment_{epoch}.h5\"),\n",
    "      keras.callbacks.ModelCheckpoint(\"models/\"+ name +\"_best.h5\", save_best_only=True, monitor=\"val_accuracy\"),\n",
    "   ]\n",
    "   return model.fit(\n",
    "      train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds,\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "\treturn x * tf.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment dataset\n",
    "inputs = keras.Input(shape=image_size + (3,))\n",
    "aug_model = data_augmentation(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 256, 256, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 62, 62, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 60, 60, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               14745856  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 73)                18761     \n",
      "=================================================================\n",
      "Total params: 14,904,041\n",
      "Trainable params: 14,904,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if aug_model is None:\n",
    "\tprint(\"No augmentation\")\n",
    "\tmodel = keras.models.Sequential()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3, 3),padding=\"same\", activation=swish, input_shape=image_size + (3,)))\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), padding=\"same\", activation=swish))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=swish))\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=swish))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Conv2D(128, (3, 3), activation=swish))\n",
    "model.add(keras.layers.Conv2D(128, (3, 3), activation=swish))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256,activation=swish))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(len(class_track), activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-3),\n",
    "      loss=\"sparse_categorical_crossentropy\",\n",
    "      metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "#model.build(input_shape=image_size + (3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainModel(model, \"augment\", epochs=20):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load example model\n",
    "model = keras.models.load_model(\"models/resenet50_trans_pooled_best.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning from ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning model\n",
    "base_model = keras.models.Sequential()\n",
    "\n",
    "pretrained_model = keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg',\n",
    "    input_shape=image_size + (3,),\n",
    ")\n",
    "\n",
    "# Freeze base model\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "base_model.add(pretrained_model)\n",
    "\n",
    "base_model.add(keras.layers.Flatten())\n",
    "base_model.add(keras.layers.Dense(512, activation=swish))\n",
    "base_model.add(keras.layers.Dense(len(class_track), activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 24,641,930\n",
      "Trainable params: 1,054,218\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our base model\n",
    "\n",
    "The ResNet50 portion is frozen, and we just train the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example model\n",
    "base_model = keras.models.load_model(\"models/resenet50_trans_pooled_best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - 38s 855ms/step - loss: 0.5998 - accuracy: 0.8263 - val_loss: 0.2776 - val_accuracy: 0.9251\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 33s 809ms/step - loss: 0.1963 - accuracy: 0.9514 - val_loss: 0.3238 - val_accuracy: 0.8801\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 33s 816ms/step - loss: 0.1407 - accuracy: 0.9748 - val_loss: 0.3073 - val_accuracy: 0.8951\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 32s 787ms/step - loss: 0.1035 - accuracy: 0.9841 - val_loss: 0.3152 - val_accuracy: 0.8989\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 32s 790ms/step - loss: 0.0698 - accuracy: 0.9925 - val_loss: 0.4119 - val_accuracy: 0.8502\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 32s 789ms/step - loss: 0.0590 - accuracy: 0.9907 - val_loss: 0.3732 - val_accuracy: 0.8689\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 33s 806ms/step - loss: 0.0532 - accuracy: 0.9935 - val_loss: 0.4783 - val_accuracy: 0.8427\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 33s 788ms/step - loss: 0.0364 - accuracy: 0.9953 - val_loss: 0.4170 - val_accuracy: 0.8427\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 33s 795ms/step - loss: 0.0587 - accuracy: 0.9841 - val_loss: 0.4431 - val_accuracy: 0.8577\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 33s 816ms/step - loss: 0.0352 - accuracy: 0.9953 - val_loss: 0.5230 - val_accuracy: 0.8052\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 33s 809ms/step - loss: 0.0420 - accuracy: 0.9925 - val_loss: 0.4863 - val_accuracy: 0.8352\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 33s 814ms/step - loss: 0.0469 - accuracy: 0.9879 - val_loss: 0.5951 - val_accuracy: 0.7903\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 34s 827ms/step - loss: 0.0540 - accuracy: 0.9916 - val_loss: 0.5202 - val_accuracy: 0.8165\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 33s 809ms/step - loss: 0.0274 - accuracy: 0.9963 - val_loss: 0.4933 - val_accuracy: 0.8277\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 35s 863ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.6943 - val_accuracy: 0.7940\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 33s 801ms/step - loss: 0.0177 - accuracy: 0.9953 - val_loss: 0.5825 - val_accuracy: 0.7978\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 33s 799ms/step - loss: 0.0225 - accuracy: 0.9935 - val_loss: 0.6792 - val_accuracy: 0.7865\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 35s 864ms/step - loss: 0.0235 - accuracy: 0.9963 - val_loss: 0.8455 - val_accuracy: 0.7678\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 34s 840ms/step - loss: 0.0146 - accuracy: 0.9972 - val_loss: 0.5886 - val_accuracy: 0.8090\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 34s 836ms/step - loss: 0.0265 - accuracy: 0.9944 - val_loss: 0.8891 - val_accuracy: 0.7416\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = trainModel(base_model, \"resenet50_trans_pooled\", 20)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.axis(ymin=0.4,ymax=1)\n",
    "plt.grid()\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're getting good accuracy (converged on the new data), we unfreeze the model and train the whole thing.\n",
    "\n",
    "Important that we use a low learning rate so we don't make big jumps and overwrite things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "model = keras.models.load_model(\"models/resenet50_trans_pooled_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/34 [==============================] - 46s 906ms/step - loss: 0.9596 - accuracy: 0.6611 - val_loss: 0.3016 - val_accuracy: 0.9064\n",
      "Epoch 2/20\n",
      "34/34 [==============================] - 34s 821ms/step - loss: 0.4668 - accuracy: 0.8739 - val_loss: 0.4238 - val_accuracy: 0.8764\n",
      "Epoch 3/20\n",
      "34/34 [==============================] - 34s 820ms/step - loss: 0.2782 - accuracy: 0.9505 - val_loss: 0.5086 - val_accuracy: 0.8464\n",
      "Epoch 4/20\n",
      "34/34 [==============================] - 34s 840ms/step - loss: 0.1766 - accuracy: 0.9851 - val_loss: 0.5762 - val_accuracy: 0.7790\n",
      "Epoch 5/20\n",
      "34/34 [==============================] - 33s 825ms/step - loss: 0.1218 - accuracy: 0.9897 - val_loss: 0.6435 - val_accuracy: 0.7753\n",
      "Epoch 6/20\n",
      "34/34 [==============================] - 33s 813ms/step - loss: 0.0884 - accuracy: 0.9907 - val_loss: 0.6952 - val_accuracy: 0.7640\n",
      "Epoch 7/20\n",
      "34/34 [==============================] - 34s 838ms/step - loss: 0.0663 - accuracy: 0.9944 - val_loss: 0.7278 - val_accuracy: 0.7528\n",
      "Epoch 8/20\n",
      "34/34 [==============================] - 33s 820ms/step - loss: 0.0497 - accuracy: 0.9963 - val_loss: 0.7578 - val_accuracy: 0.7378\n",
      "Epoch 9/20\n",
      "34/34 [==============================] - 34s 847ms/step - loss: 0.0460 - accuracy: 0.9944 - val_loss: 0.7760 - val_accuracy: 0.7303\n",
      "Epoch 10/20\n",
      "34/34 [==============================] - 34s 836ms/step - loss: 0.0361 - accuracy: 0.9953 - val_loss: 0.7893 - val_accuracy: 0.7303\n",
      "Epoch 11/20\n",
      "34/34 [==============================] - 33s 827ms/step - loss: 0.0333 - accuracy: 0.9963 - val_loss: 0.8031 - val_accuracy: 0.7266\n",
      "Epoch 12/20\n",
      "34/34 [==============================] - 35s 857ms/step - loss: 0.0252 - accuracy: 0.9972 - val_loss: 0.8157 - val_accuracy: 0.7228\n",
      "Epoch 13/20\n",
      "34/34 [==============================] - 35s 864ms/step - loss: 0.0252 - accuracy: 0.9963 - val_loss: 0.8246 - val_accuracy: 0.7228\n",
      "Epoch 14/20\n",
      "34/34 [==============================] - 35s 855ms/step - loss: 0.0248 - accuracy: 0.9935 - val_loss: 0.8376 - val_accuracy: 0.7154\n",
      "Epoch 15/20\n",
      "34/34 [==============================] - 34s 830ms/step - loss: 0.0194 - accuracy: 0.9963 - val_loss: 0.8456 - val_accuracy: 0.7191\n",
      "Epoch 16/20\n",
      "34/34 [==============================] - 35s 850ms/step - loss: 0.0206 - accuracy: 0.9944 - val_loss: 0.8506 - val_accuracy: 0.7191\n",
      "Epoch 17/20\n",
      "34/34 [==============================] - 34s 836ms/step - loss: 0.0169 - accuracy: 0.9972 - val_loss: 0.8586 - val_accuracy: 0.7041\n",
      "Epoch 18/20\n",
      "34/34 [==============================] - 35s 867ms/step - loss: 0.0170 - accuracy: 0.9953 - val_loss: 0.8653 - val_accuracy: 0.7116\n",
      "Epoch 19/20\n",
      "34/34 [==============================] - 35s 858ms/step - loss: 0.0175 - accuracy: 0.9953 - val_loss: 0.8670 - val_accuracy: 0.6966\n",
      "Epoch 20/20\n",
      "34/34 [==============================] - 36s 889ms/step - loss: 0.0129 - accuracy: 0.9963 - val_loss: 0.8707 - val_accuracy: 0.7079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x280114bec70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.trainable = True\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "trainModel(model, \"resenet50_transfer_ete\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model\n",
    "\n",
    "# load an image\n",
    "\n",
    "def print_image(model, img):\n",
    "\timg_array = keras.preprocessing.image.img_to_array(img)\n",
    "\timg_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "\n",
    "\tpredictions = model.predict(img_array)\n",
    "\tfor i in np.argpartition(predictions[0], -3)[-3:]:\n",
    "\t\tprint(class_track[i] + \": \" + str(predictions[0][i]))\n",
    "\tprint(\"---\")\n",
    "\n",
    "\n",
    "images = [\n",
    "\t\"images/2pvptp6z5ul81.jpg\",\n",
    "\t\"images/3o3mq3j5q1j81.jpg\",\n",
    "\t\"images/35w8x7t3tz761.png\",\n",
    "\t\"images/BeWN228.png\",\n",
    "\t\"images/carj0fmcxei81.png\",\n",
    "\t\"images/wyn90upnrwm81.jpg\",\n",
    "\t\"images/yvxknmz03di81.jpg\",\n",
    "]\n",
    "\n",
    "\n",
    "#Load the model\n",
    "model = keras.models.load_model(\"models/resenet50_trans_pooled_v2.h5\")\n",
    "\n",
    "for img_path in images:\n",
    "\timg = keras.preprocessing.image.load_img(\n",
    "\t\timg_path, target_size=image_size\n",
    "\t)\n",
    "\tprint (img_path)\n",
    "\tprint_image(model, img)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1122e89fe43e8dd2cd7ffbec99af0a55fb1bf71a12cfdaf83edd54c3b1d7c661"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
